4 Analysing the cryptographic primitives outputs
In the chapter we present the results of two distinct exeriments and describe
the methods and battery configurations used for obtaining the results.
In the first experiment, we analysed large quantities of random data
with the statistical batteries implemented in RTT. The motivation for the
experiment was that the results of the multiple repeated analyses would
provide us with a reference for our further experiments. Our subsequent
experiments would differ from the baseline experiment only in the input
data but not in the settings of the respective batteries.
In the second experiment, outputs of 15 distinct cryptographic algorithms
were analysed. The specific algorithms were chosed based either on their
popularity or on their success in cryptographic competitions eSTREAM
(cit.) and SHA3 (cit.); among the chosen functions are algorithms such as
AES, DES, RC4 or Keccak. The results of the experiment were compared
with results of the analysis in EACirc framework. The EACirc framework
implements alternative approach to the randomness testing and is developed
in CRoCS (cit.).
4.1 Battery configurations used in the experiments
In this section we will present the configurations of the respective batteries
that were used in both above-mentioned experiments. The settings were kept
as close as possible to the default settings of the respective batteries; because
of this, the results of the batteries executed through RTT are comparable to
the results of the batteries when directly executed. The battery configuration
file that was used in RTT is in Appendix F.
4.1.1 Common settings
All batteries were provided 8000 MiB 1 long data files for the analysis in each
execution. The batteries were not required to process the entirety of the data;
however rewinding the file and reading more bytes was not permitted to
the batteries. The file size was chosen as a compromise between the amount
of data needed by the separate tests in the batteries and our capabilities to
process and store such quantities of data.
1. The exact length of single file was 8000 × 1024 2 = 8388608000 bytes.
234. Analysing the cryptographic primitives outputs
NIST Statistical Testing Suite
The settings of the tests in NIST STS was the same as in default execution
of the battery. The tests along with their default settings are listed in Ap-
pendix A. In single run, each test processed 1000 data streams of size 1000000
bits; in total, each test analysed 125 MB of data in its run.
Dieharder
The test settings of the Dieharder battery follow default settings outlined in
Appendix B with the single exception of test RGB Lagged Sums. In default
settings, the test would need too much data 2 for the analysis.
The RGB Lagged Sum test is executed in 33 variants with variable pa-
rameter n={0..32}. The number of bytes the test variant will process scales
with the parameter n of the variant and number of times the test variant
will be repeated (p-samples) during the battery execution. To lower the
amount of the needed bytes, we reduced the number of repetitions for each
respective variant based on its n. The number of repetitions was calculated
using following function.
8000 × 1024 2
Rep ( n ) = min 100;
( n + 1 ) × 4 × 10 6



TestU01
The batteries Small Crush and Crush are executed in their default settings.
The default settings of the tests in their respective batteries can be found in
TestU01 documentation (cit.).
The battery Big Crush is omitted from our analysis because the tests in
the battery would need at least 60 GB of data to be fully executed which is
an infeasible amount.
The Rabbit, Alphabit and Block Alphabit batteries are executed in default
test settings. The tests are limited to 8000 MiB of data by the parameter nb.
The tests in the batteries Alphabit and Block Alphabit have settings that
would cause them to process only certain parts of each 32 bit block in the
data. These settings are not used and all bits in 32 bit blocks are analysed
by the tests. Additionally, all tests from Block Alphabit are executed in 6
different variants; each test variant analyses differently reordered blocks of
32 bits.
2. RGB Lagged Sum test with parameter n=32 will analyse 13.2 GB of data.
244. Analysing the cryptographic primitives outputs
4.2 Baseline experiment
Aim of this experiment was to observe behavior and results of the batteries
in RTT when analysing truly random data. The observation would be then
used for the interpretation of results of our other experiments.
The evaluation of the battery result is based on the number of failed tests
in the battery. Under the assumption that we have single second-level p-
value per single test, the test is considered failed when its p-value is outside
of the interval [ α, 1 ) and its null hypothesis H 0 is rejected. The reason why
we chose such interval is explained is Section 2.3.1.
When analysing truly random data repeatedly with a statistical test, we
will observe that the resulting p-values are uniformly distributed on the
interval [ 0, 1 ] . The uniformity is caused by the fact that we can’t predict out-
put of a TRNG and each possible output is observed with equal probability.
Therefore we also can’t predict result of the analysis with a statistical test.
Since the p-values are uniformly distributed, they will fall outside of the
interval [ α, 1 ) with probability equal to α; hence test’s H 0 is rejected and
the test fails with rate equal to significance level α. Rejection of H 0 , when it
actually holds true, is called Type I error (cit.). The theoretical probability
of Type I error should be equal to the value of the significance level.
When we are evaluating a battery analysis based on number of failed
tests, we must know how many tests we expect to fail when the H 0 holds
true. We can calculate the expected value like so. Let n be the number of
tests in a battery. Then we have n independent experiments that will result
in either yes (failure of the test) with probability α or no (success of the test)
with probability 1 − α. This situation can be modeled by using binomial
distribution (cit.) Bin ( n, α ) . Let F Exp be a random discrete variable expressing
number of failed tests in a battery with expected distribution Bin ( n, α ) . From
the definition of the binomial distribution, the expected number of the failed
tests is E ( F Exp ) = n × α. Therefore, should a battery finish with n × α failed
tests we still can’t reject the H 0 for the whole battery; we will treat the data
analysis as successful.
However, the expected number of failures is not necesarilly exact, as the
above-mentioned calculation treats tests as a independent trials. Separate
tests in a battery are not required nor proved to be independent, which can
lead to differing distributions of empirical and theoretical random variables
representing number of failed tests.
To observe empirical distribution of the number of failed tests F Obs we
inspected the number of failures of the respective batteries during analysis
of truly random data. In total, approximately 8 terabytes of random data
were downloaded from QRNG service provided by Humboldt-Universitaet
zu Berlin (cit.). The data were split into 1000 blocks each of size 8000 GiB.
All 7 batteries included in this experiment analysed each block of data once.
254. Analysing the cryptographic primitives outputs
Sample size of 1000 trials is enough for us to compare the distributions of
the two random variables F Exp and F Obs .
The counts of failures of respective batteries were inspected in following
two scenarios.
Original second-level p-values failures
In this scenario, we assumed that all second-level p-values produced by a
specific battery are independent. Therefore, all statistics calculated by the
battery were treated as separate tests. For example, if some test is executed
in two variants, each of those variants has two subtests and each subtest will
calculate 2 statistics, in total we will get 2 3 = 8 second-level p-values; we
consider these eight p-values as the results of eight independent tests each
of whose calculated single p-value.
Corrected p-values failures
Whit this second scenario, we tried to mitigate the effect of dependent p-
values in our set by grouping them together. We put the p-values that are
most likely dependent into single set or group. We assumed that the p-values
that are most likely inter-connected are those that are produced by a single
statistical test. Therefore, all p-values belonging to a single test were reduced
to only one p-value, leaving us with a single result per distinct test in a
statistical battery. For example, should a single test produce eight p-values,
for this test we will only have information whether the test was a failure or
not. For the grouping of the p-values we used Šidák’s correction (cit.) and
the process is shown in Definition 4.2.1.
Definition 4.2.1. For n grouped p-values P = { p 1 , .., p n } with significance
level α and null hypothesis H 0 we will calculate the corrected significance
1
level α 0 = 1 − ( 1 − α ) n . If ∃ p ∈ P : p < α 0 , then H 0 is rejected for ∀ p ∈ P
and the whole group result is treated as a failure.
To compare the feasibility of the two above-mentioned scenarios, we
needed to compare both of the observed distributions to their respective
expected distributions and see how they perform. We consider the approach
more feasible when it is less differing from its expected distribution; the
battery results are easier to interpret when they are closer to the theoretical
expectations. The comparison of the distributions was realised by following
steps.
Definition 4.2.2. Let X = { x 1 , .., x n } be a multiset of n elements. Then we
define frequency variable f e X as the frequency of element e in multiset X. If
@ e, x ∈ X : e = x, then f e X = 0.
During this experiment each battery was executed 1000 times; for each
battery we had s = 1000 samples of a discrete random variable F Obs , ex-
pressing the number of failed tests in that battery. Random variable F Exp
264. Analysing the cryptographic primitives outputs
expresses theoretical expected number of failures in a battery with n test and
significance level α. Variable F Exp follows binomial distribution Bin ( n, α ) .
The probability of observing x test failures (written as F Exp = x) in Bin ( n, α )
is defined with probability mass function of binomial distribution (Equa-
tion 4.1).
P ( F Exp
 
n
= x ) =
× α x × ( 1 − α ) n − x
x
(4.1)
Let X = { x 1 , .., x s } be a multiset of the samples of F Obs and D = { d 1 , .., d k }
set of all k distinct values in X. Then f d X , d ∈ D (see Definition 4.2.2) repre-
sents a random variable expressing the frequency of a failure count within a
multiset X. The random variable f d X is expected to follow binomial distribu-
tion Bin ( s, p ) where s is the number of samples and p is the probability of
observing d failures (P ( F Exp = d ) , Equation 4.1). The expected value of the
random variable f d X is defined using expected value of binomial distribution
(Equation 4.2).
 
E f d X = P ( F Exp = d ) × s
(4.2)
Now that we have both observed frequencies f d X and their expected

values E f d X for d ∈ D we can assess their closeness. For this assessment
we used Pearson’s chi-squared test (χ 2 ) that evaluates that any observed
difference between two data sets (e.g. observed and expected frequencies of
failure counts) happened by chance (cit.). We calculated Pearson’s cumula-
tive test statistic χ 2 for each pair of expected and observed frequencies as
shown in Equation 4.3.
χ 2 =
k
∑
i = 1

  2
f d X − E f d X
i
  i
E f d X
(4.3)
i
Due to the limitation of χ 2 test stating that the expected value must
be at least 5, the samples that didn’t satisfy this condition were summed
together and treated as a single sample. We then calculated p-value using
the χ 2 distribution (cit.). The p-value calculation is based on the degrees of
freedom that are equal to the size of the tested set minus one (in our case
k − 1) and on the value of χ 2 statistic. The p-value can be then obtained by
using one of the various online tools (cit.).
The resulting p-value is indicator of how close are the two tested datasets.
The p-value expresses the probability that any difference between the two
tested sets happened by chance. Therefore, when we are comparing the
feasibility of the two failure counting scenarios (original versus corrected
results) the more feasible one will have higher p-value than the other; our
274. Analysing the cryptographic primitives outputs
point was to find out which one of our approaches followed our theoretical
expectations more closely.In Figure 4.1 we can see both original and corrected failure frequecies
along with their expected distributions. As the χ 2 statistic p-value suggest,
in the corrected version of failure counting, the observed failure frequency
is much closer to the expected distribution than in the original version.In Table 4.2 we present comparison of the χ 2 statistic p-values between
the original and the corrected results. We can see, that in general, the cor-
rected results are closer to their expected distributions. Only in two cases
(NIST STS and Rabbit), the corrected results perform worse while in the
remaining batteries they offer a significant improvement. We also present
the acceptance bounds for the batteries. The acceptance bound represent
the maximum number of failed tests in a battery, while not rejecting the null
284. Analysing the cryptographic primitives outputs
hypothesis. Should the number of failures exceed the acceptance bound, we
will reject H 0 ; the tested data failed to pass the battery.
However, even with the improved result interpretation, some failure
frequencies are still too distant from their theoretical distributions. For
this reason, we decided to increase the acceptance bounds. The acceptance
bounds are partly based on maximum number of observed failures in the
battery and partly on the probability of observing such number of failures
(see Equation 4.1). In all cases, the probability of seeing the number of
failures equal to acceptance bound is not lesser than 0.001.
The presented bounds are used for result interpretation of the subsequent
experiments. If the number of failed tests will be higher than the bound, the
data did not pass the battery analysis.
4.3 Analysis of well-known cryptographic algorithms
In this experiments we focused on the analysis of well-known and used
cryptographic primitives. The 15 algorithms were chosen based on their
popularity and success in cryptographic competitions eSTREAM (cit.) and
SHA3 (cit.).
Our motivation for this experiment was to find out how big are the
security margins on common algorithms. Most of the tested algorithms
could be reduced in number of their rounds. We wanted to see how much
could we reduce the rounds, until we would start detecting the biases in the
data.
The outputs of the functions were generated using the generator devel-
oped in CRoCS (cit.). The plaintext of the functions was binary counter,
the key was set to random data and initialization vector to zeroes. Not
all functions needed the key and IV. For each specific round and function
combination we generated 8000 MiB long data file. In total, 72 distinct com-
binations of functions and rounds were analysed. The configurations of the
batteries were the same as described in Section 4.1.
The results of this experiment were also compared to the results pro-
duced by the EACirc framework (cit.). The results of the analysis of the data
with EACirc are part of Karel Kubíček’s diploma thesis (cit.).
4.3.1 Results of the analysis
The results of the analysis are shown in Table 4.3. We included only the
combinations of algorithms and rounds in which the data failed to pass
either EACirc or any battery in RTT. The marked cells in the table indicate
failure of the battery.The values in the cells express the number of failed tests in a specific
battery. Each battery has defined total number of tests in the header. In some
cases, not all tests in the battery could be executed; in these cases we specify
the number of executed tests in the cell.
We also provide security margins of the analysed algorithms. The secu-
rity margins are based on the highest round in which the algorithm failed to
pass any battery and on the default total number of rounds in the algorithm.
304. Analysing the cryptographic primitives outputs
4.3.2 Detection of specific biases
Higher rounds of the algorithms marked by an asterisk were included in
the table because after further analysis of the results, bias was found in the
included rounds.
4.3.3 Notable results
4.3.4 Discussion
29
